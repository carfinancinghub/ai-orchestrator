import glob, json, os, time, logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

REPORTS_ROOT = Path(os.getenv('REPORTS_ROOT', str(Path(__file__).resolve().parent.parent / 'reports')))

EXCLUDE_FILES = {
  'cypress.config.js', 'vite.config.js', 'vite.config.ts', 'jest.config.js'
}

@dataclass
class Candidate:
    repo: str
    branch: str
    src_path: str
    mtime: float

def _path_is_test_or_mock(path_str: str) -> bool:
    p = path_str.replace('\\\\','/').lower()
    if '/__mocks__/' in p: return True
    base = os.path.basename(p)
    if base.endswith(('.test.js','.test.jsx','.spec.js','.spec.jsx','.test.ts','.test.tsx','.spec.ts','.spec.tsx')):
        return True
    return False

def _path_is_excluded_name(path_str: str) -> bool:
    base = os.path.basename(path_str)
    return base in EXCLUDE_FILES

def _rel_from_root(abs_path: str, root: Path) -> str:
    ap = abs_path.replace('\\\\','/').lower()
    rp = str(root).replace('\\\\','/').lower()
    if not rp.endswith('/'): rp += '/'
    if ap.startswith(rp):
        return ap[len(rp):]
    return ap

def _should_include_jsjsx(rel_path: str, prefixes: List[str]) -> Tuple[bool, str]:
    # Only src/**/*.js|jsx
    if not rel_path.endswith(('.js','.jsx')):
        return False, 'skip: not js/jsx'
    if any(rel_path.endswith(ext) for ext in ('.test.js','.test.jsx','.spec.js','.spec.jsx')):
        return False, 'skip: test file'
    if '/__mocks__/' in rel_path:
        return False, 'skip: mocks'
    if _path_is_excluded_name(rel_path):
        return False, f'skip: excluded config ({os.path.basename(rel_path)})'
    if prefixes and not any(rel_path.startswith(pref) for pref in prefixes):
        return False, f'skip: prefix mismatch (want {prefixes})'
    return True, 'include'

def _glob_reports(patterns: List[str]) -> List[str]:
    out: List[str] = []
    for pat in patterns:
        out.extend(glob.glob(pat, recursive=True))
    return out

def fetch_candidates(
    org: Optional[str],
    repo_name: Optional[str],
    platform: str,
    token: str,
    run_id: str,
    branches: Optional[List[str]],
    local_inventory_paths: Optional[List[str]] = None,
    user: Optional[str] = None,
):
    logger.info(f"Starting fetch_candidates for run_id {run_id}")
    t0 = time.time()
    candidates: List[Candidate] = []
    files: List[Dict] = []
    exclusions: List[Dict] = []

    # Prefix filter
    prefixes_env = os.getenv('CFH_SCAN_PATH_PREFIXES', 'src/')
    path_prefixes = [p.strip().lower() for p in prefixes_env.split(',') if p.strip()]
    logger.info(f"Using path prefix filter(s): {path_prefixes}")

    # Inventory discovery
    cache_file = REPORTS_ROOT / f"inventory_cache_{run_id}.json"
    if cache_file.exists():
        logger.info(f"Removing stale inventory cache: {cache_file}")
        cache_file.unlink()

    if local_inventory_paths is None:
        local_inventory_paths = _glob_reports([
            str(REPORTS_ROOT / 'inventory*.txt'),
            str(REPORTS_ROOT / 'inventory*.list'),
            str(REPORTS_ROOT / 'inventory*.csv'),
            str(REPORTS_ROOT / 'inventory*.json'),
            str(REPORTS_ROOT / 'inventories/*.*'),
            str(REPORTS_ROOT / 'file_scan_results_*.md'),
        ])
        logger.info(f"Auto-detected local inventory paths: {local_inventory_paths}")

    # Default root: local frontend repo
    default_root = Path('C:/Backup_Projects/CFH/frontend')
    root_dir = default_root
    if local_inventory_paths:
        # If path is a dir, use it as root; if it's a file, use its parent
        p0 = Path(local_inventory_paths[0])
        root_dir = p0 if p0.is_dir() else p0.parent
    logger.info(f"Scanning root directory: {root_dir}")

    # Local scan
    for p in root_dir.rglob('*'):
        if not p.is_file(): continue
        abs_path = str(p).replace('\\\\','/').lower()
        rel = _rel_from_root(abs_path, root_dir)
        if _path_is_excluded_name(rel): 
            exclusions.append({'path': rel, 'reason': 'excluded config'}); 
            continue
        if _path_is_test_or_mock(rel): 
            exclusions.append({'path': rel, 'reason': 'test/mock'}); 
            continue
        # Track TS/TSX in inventory but do not include as conversion candidates
        if rel.endswith(('.ts','.tsx')):
            files.append({'repo': 'local', 'branch': 'main', 'path': rel})
            continue
        ok, reason = _should_include_jsjsx(rel, path_prefixes)
        if not ok:
            exclusions.append({'path': rel, 'reason': reason})
            continue
        files.append({'repo':'local','branch':'main','path':rel})
        candidates.append(Candidate(repo='local', branch='main', src_path=rel, mtime=p.stat().st_mtime))
        logger.info(f"Local candidate: {rel}")

    # GitHub scan (unchanged except: match prefixes on LOWER-CASED RELATIVE PATHS)
    repos_info = []
    if platform == 'github':
        try:
            from github import Github, GithubException
        except Exception as e:
            logger.error(f"PyGitHub not available: {e}")
            Github = None; GithubException = Exception  # type: ignore
        user_or_org = user or 'carfinancinghub'
        if Github:
            gh = Github(token)
            # list_repos_and_branches() left as-is; ensure branch refs iterate
            # For brevity here, we recommend keeping your existing GitHub logic but
            # apply the same prefix/test/mock/exclude filters to sub_file.path.lower().
            # (Omitted to keep this patch focused.)
            pass

    # Emit reports
    payload = {
        'run_id': run_id,
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'user': user, 'org': org, 'repo_name': repo_name, 'branches': branches,
        'found_total': len(files),
        'found_js_jsx': len([f for f in files if f['path'].endswith(('.js','.jsx'))]),
        'candidates': [{'repo': c.repo, 'branch': c.branch, 'path': c.src_path, 'mtime': c.mtime} for c in candidates],
        'exclusions': exclusions,
    }
    REPORTS_ROOT.mkdir(parents=True, exist_ok=True)
    (REPORTS_ROOT / f'scan_{run_id}.json').write_text(json.dumps(payload, indent=2), encoding='utf-8')
    (REPORTS_ROOT / 'scan_latest.json').write_text(json.dumps(payload, indent=2), encoding='utf-8')
    # Regenerate tsx_inventory_full_clean.csv (only js/jsx under src/)
    inv_csv = REPORTS_ROOT / 'tsx_inventory_full_clean.csv'
    try:
        import csv
        with inv_csv.open('w', encoding='utf-8', newline='') as fp:
            w = csv.writer(fp)
            w.writerow(['path'])
            for f in files:
                p = f['path']
                if p.startswith('src/') and p.endswith(('.js','.jsx')): w.writerow([p])
        logger.info(f'Wrote {inv_csv}')
    except Exception as e:
        logger.warning(f'Could not write {inv_csv}: {e}')
    logger.info(f"Completed fetch_candidates in {time.time()-t0:.2f}s")
    return candidates, {}, []